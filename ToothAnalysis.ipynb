{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scanned *a lot* of teeth for the ZMK guys and gals.\n",
    "With this notebook we prepare the tomographic datasets for display and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "from numcodecs import Blosc\n",
    "from tqdm import notebook\n",
    "import skimage.filters\n",
    "import skimage.morphology\n",
    "import skimage.segmentation\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You can seee what DASK is doing at \"http://localhost:%s/status\"' % client.scheduler_info()['services']['dashboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/62242245/323100\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "# Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')\n",
    "# Resize figures\n",
    "plt.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_hash():\n",
    "    '''\n",
    "    Get the current git hash from the repository.\n",
    "    Based on http://stackoverflow.com/a/949391/323100 and\n",
    "    http://stackoverflow.com/a/18283905/323100\n",
    "    '''\n",
    "    from subprocess import Popen, PIPE\n",
    "    import os\n",
    "    gitprocess = Popen(['git',\n",
    "                        '--git-dir',\n",
    "                        os.path.join(os.getcwd(), '.git'),\n",
    "                        'rev-parse',\n",
    "                        '--short',\n",
    "                        '--verify',\n",
    "                        'HEAD'],\n",
    "                       stdout=PIPE)\n",
    "    (output, _) = gitprocess.communicate()\n",
    "    return output.strip().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for output\n",
    "OutPutDir = os.path.join(os.getcwd(), 'Output', get_git_hash())\n",
    "print('We are saving all the output to %s' % OutPutDir)\n",
    "os.makedirs(OutPutDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 9\n",
    "# And then do something like\n",
    "# plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = False\n",
    "# to speed things up significantly\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.sep, 'home', 'habi', '1272')\n",
    "else:\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        if 'anaklin' in platform.node():\n",
    "            BasePath = os.path.join('S:\\\\')\n",
    "        else:\n",
    "            BasePath = os.path.join('D:\\\\Results')\n",
    "Root = os.path.join(BasePath, 'ZMK')\n",
    "b0rkpath = False  # Change path to something wrong, so we can test the download feature\n",
    "if b0rkpath:\n",
    "    Root = 'SOMEWHERE WRONG'\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Linux' in platform.system():\n",
    "    tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\')\n",
    "dask.config.set({'temporary_directory': os.path.join(tmp, 'tmp')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look only for folders: https://stackoverflow.com/a/38216530\n",
    "Data['Folder'] = glob.glob(os.path.join(Root, 'ToothBattallion', '*' + os.path.sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(Data):\n",
    "#     # Temporarily use a subset of the data\n",
    "#     # By setting a random state we always get the same subset, which is nice for the publication...\n",
    "# #     Data = Data.sample(n=24, random_state=1796) # As in manuscript\n",
    "#     Data = Data.sample(n=24, random_state=333)  # So that it contains Tooth045\n",
    "#     Data.reset_index(drop=True, inplace=True)\n",
    "#     print('We are currently working with a subset of %s teeth' % len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not len(Data):\n",
    "    # Our dataframe is empty.\n",
    "    # We might be running on Binder, i.e. offer to download some sample data :)\n",
    "    import requests\n",
    "    # Inform user\n",
    "    print('You are most probably running the notebook on binder and thus do not have access to the data files')\n",
    "    print('We are downloading and unpacking two of them for you...')\n",
    "    # Change root folder\n",
    "    Root = 'data'\n",
    "    os.makedirs(Root, exist_ok=True)\n",
    "    # Download two files with the reconstructions of two teeth\n",
    "    filelist = ['https://osf.io/7trgk/download', 'https://osf.io/fqm4x/download']\n",
    "    ziplist = ['7.zip', '45.zip']\n",
    "    for c, file in enumerate(filelist):\n",
    "        if os.path.exists(os.path.join(Root, ziplist[c])):\n",
    "            print(os.path.join(Root, ziplist[c]), 'already downloaded')\n",
    "        else:\n",
    "            # Progress bar for download via https://stackoverflow.com/a/44920494/323100\n",
    "            download = requests.get(file, stream=True)\n",
    "            totalsize = int(download.headers[\"Content-Length\"])\n",
    "            chunksize = 1024 * 1024\n",
    "            bars = int(totalsize / chunksize)\n",
    "            with open(os.path.join(Root, ziplist[c]), \"wb\") as f:\n",
    "                for chunk in notebook.tqdm(download.iter_content(chunk_size=chunksize),\n",
    "                                           total=bars,\n",
    "                                           unit=\"MB\",\n",
    "                                           desc='Downloading %s (%s GB)' % (ziplist[c],\n",
    "                                                                            round(totalsize * 1e-9, 1))):\n",
    "                    f.write(chunk)\n",
    "    # Unzip the files\n",
    "    import zipfile\n",
    "    ziplist = glob.glob(os.path.join(Root, '*.zip'))\n",
    "    for downloaded_file in notebook.tqdm(ziplist,\n",
    "                                         total=len(ziplist),\n",
    "                                         desc='Unzipping %s files' % len(ziplist)):\n",
    "        with zipfile.ZipFile(downloaded_file, 'r') as zip_file:\n",
    "            for file in notebook.tqdm(iterable=zip_file.namelist(),\n",
    "                                      desc=file,\n",
    "                                      total=len(zip_file.namelist()),\n",
    "                                      leave=False):\n",
    "                if not os.path.exists(os.path.join(Root, file)):\n",
    "                    zip_file.extract(member=file, path=Root)\n",
    "    # Read in what we downloaded, now len(Data) should not be zero :)\n",
    "    Data['Folder'] = glob.glob(os.path.join(Root, '*' + os.path.sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We work with %s tooth folders in %s' % (len(Data), Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could do it in a list comprehension, but then it fails if we're still scanning a tooth\n",
    "# Data['LogFile'] = [sorted(glob.glob(os.path.join(f, '*.log')))[0] for f in Data['Folder']]\n",
    "for c, row in Data.iterrows():\n",
    "    try:\n",
    "        Data.at[c, 'LogFile'] = sorted(glob.glob(os.path.join(row['Folder'], 'proj', '*.log')))[0]\n",
    "    except IndexError:\n",
    "        print('No logfile found in %s, removing the folder temporarily' % row.Folder)\n",
    "        Data.at[c, 'LogFile'] = 'scanning'\n",
    "Data = Data[Data['LogFile'] != 'scanning']\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s tooth folders to work with' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Sample'] = [os.path.splitext(os.path.basename(logfile))[0] for logfile in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper sorting *with* leading zeros :)\n",
    "Data.sort_values(by=['Sample'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names of the reconstructions\n",
    "Data['Reconstructions'] = [sorted(glob.glob(os.path.join(folder,\n",
    "                                                         'rec',\n",
    "                                                         '*rec*.png'))) for folder in Data['Folder']]\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data.Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have not been reconstructed yet\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "for sample in Data[Data['Number of reconstructions'] == 0]['Sample']:\n",
    "    print('%s has not been reconstructed yet, we remove it from our data temporarily' % sample)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s tooth folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In total, we have',\n",
    "      Data['Number of reconstructions'].sum(),\n",
    "      'reconstructions over all the 104 datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('On average, each tooth has about',\n",
    "      int(round(Data['Number of reconstructions'].mean())),\n",
    "      'reconstructions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixelsize(logfile):\n",
    "    \"\"\"Get the pixel size from the scan log file\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Image Pixel' in line and 'Scaled' not in line:\n",
    "                pixelsize = float(line.split('=')[1])\n",
    "    return(pixelsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Voxelsize'] = [get_pixelsize(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check voxel sizes (*rounded* to two after-comma values)\n",
    "# If different, spit out which values\n",
    "if len(Data['Voxelsize'].round(2).unique()) > 1:\n",
    "    print('We scanned the teeth with %s different voxel sizes' % len(Data['Voxelsize'].round(2).unique()))\n",
    "    for vs in sorted(Data['Voxelsize'].round(2).unique()):\n",
    "        print('-', vs, 'um for Samples ', end='')\n",
    "        for c, row in Data.iterrows():\n",
    "            if float(vs) == round(row['Voxelsize'], 2):\n",
    "                print(row.Sample, end=', ')\n",
    "        print('')\n",
    "else:\n",
    "    print('We scanned all datasets with equal voxel size, namely %s.' % Data['Voxelsize'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction_grayvalue(logfile):\n",
    "    \"\"\"How did we map the brightness of the reconstructions?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Maximum for' in line:\n",
    "                grayvalue = float(line.split('=')[1])\n",
    "    return(grayvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacks(logfile):\n",
    "    \"\"\"How many stacks/connected scans did we make?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'conn' in line:\n",
    "                stacks = int(line.split('=')[1])\n",
    "    return(stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Stacks'] = [get_stacks(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scantime(logfile):\n",
    "    \"\"\"How long did we scan?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Scan duration' in line:\n",
    "                time = line.split('=')[1]\n",
    "    return(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Scan time'] = [get_scantime(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numproj(logfile):\n",
    "    \"\"\"How many projections did we record?\"\"\"\n",
    "    with open(logfile, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Number' in line and 'Files' in line:\n",
    "                numproj = int(line.split('=')[1])\n",
    "    return(numproj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Number of Projections'] = [get_numproj(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Grayvalue'] = [get_reconstruction_grayvalue(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Data['Grayvalue'].unique()) > 1:\n",
    "    print('We reconstructed the datasets with different maximum gray values, namely')\n",
    "    for gv in Data['Grayvalue'].unique():\n",
    "        print(gv, 'for Samples ', end='')\n",
    "        for c, row in Data.iterrows():\n",
    "            if float(gv) == row['Grayvalue']:\n",
    "                print(row.Sample, end=', ')\n",
    "        print('')\n",
    "else:\n",
    "    print('We reconstructed all datasets with equal maximum gray value, namely %s.' % Data['Grayvalue'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['PreviewImagePath'] = [sorted(glob.glob(os.path.join(f, 'rec', '*.bmp'))) for f in Data['Folder']]\n",
    "Data['PreviewImage'] = [imageio.imread(pip[0])\n",
    "                        if pip\n",
    "                        else numpy.random.random((100, c, row100)) for pip in Data['PreviewImagePath']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = (10, 20)\n",
    "lines = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    plt.subplot(lines, numpy.ceil(len(Data) / float(lines)), c + 1)\n",
    "    plt.imshow(row.PreviewImage)\n",
    "    plt.title(row.Sample)\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color='white'))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5)\n",
    "plt.savefig(os.path.join(OutPutDir, 'ScanOverviews.png'),\n",
    "            transparent=True,\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reconstructions into a DASK array and save them to disk\n",
    "# Partially based on http://stackoverflow.com/a/39195332/323100\n",
    "# and on /LungMetastasis/HighResolutionScanAnalysis.ipynb\n",
    "Data['OutputNameRec'] = [os.path.join(f, sample + '_rec.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Converting reconstructions to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameRec']):\n",
    "        print('%3s/%3s: Reading %s reconstructions and saving to %s' % (c + 1,\n",
    "                                                                        len(Data),\n",
    "                                                                        row['Number of reconstructions'],\n",
    "                                                                        row['OutputNameRec'][len(Root):]))\n",
    "        Reconstructions = dask_image.imread.imread(os.path.join(row['Folder'], 'rec', '*rec*.png'))\n",
    "        Reconstructions.to_zarr(row['OutputNameRec'],\n",
    "                                overwrite=True,\n",
    "                                compressor=Blosc(cname='zstd',\n",
    "                                                 clevel=3,\n",
    "                                                 shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in Data['OutputNameRec']:\n",
    "#     print(file)\n",
    "#     dask.array.from_zarr(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reconstructions as zarr arrays\n",
    "Reconstructions = [dask.array.from_zarr(file) for file in Data['OutputNameRec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial',\n",
    "              'Coronal',\n",
    "              'Sagittal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropper(image,\n",
    "            despeckle=True,\n",
    "            verbose=False,\n",
    "            threshold=50):\n",
    "    '''Crop array to biggest item in it'''\n",
    "    dimensions = len(image.shape)\n",
    "    if verbose:\n",
    "        print('Cropping %s-dimensional image' % dimensions)\n",
    "    # Threshold\n",
    "    thresholded = image > threshold\n",
    "    if despeckle:\n",
    "        if verbose:\n",
    "            print('Removing small objects')\n",
    "        despeckled = skimage.util.apply_parallel(skimage.morphology.remove_small_objects,\n",
    "                                                 thresholded,\n",
    "                                                 extra_keywords={'min_size': 10**dimensions},\n",
    "                                                 chunks=[15, 15, 15])\n",
    "    # Find biggest object\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.find_objects.html\n",
    "    if verbose:\n",
    "        print('Finding objects')\n",
    "    if despeckle:\n",
    "        cropdimensions = scipy.ndimage.find_objects(despeckled)[0]\n",
    "    else:\n",
    "        cropdimensions = scipy.ndimage.find_objects(thresholded)[0]\n",
    "    if verbose:\n",
    "        print('Cutting the image down to')\n",
    "        for c, cd in enumerate(cropdimensions):\n",
    "            print('\\t- %s: %s' % (c, cd))\n",
    "        if dimensions > 2:\n",
    "            # Calculate cardinal direction MIPs\n",
    "            MIPs = [image.max(axis=d) for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                                                        desc='Calculating MIPs',\n",
    "                                                                        total=len(directions))]\n",
    "            for d, direction in enumerate(directions):\n",
    "                plt.subplot(1, dimensions, d + 1)\n",
    "                plt.imshow(MIPs[d], alpha=0.618)\n",
    "                plt.imshow(dask.array.ma.masked_less(MIPs[d] > threshold, 1),\n",
    "                           alpha=0.309,\n",
    "                           cmap='viridis_r')\n",
    "                if d == 0:\n",
    "                    plt.axhline(cropdimensions[1].start,\n",
    "                                c=seaborn.color_palette()[0],\n",
    "                                label=cropdimensions[1].start)\n",
    "                    plt.axhline(cropdimensions[1].stop,\n",
    "                                c=seaborn.color_palette()[1],\n",
    "                                label=cropdimensions[1].stop)\n",
    "                    plt.axvline(cropdimensions[2].start,\n",
    "                                c=seaborn.color_palette()[2],\n",
    "                                label=cropdimensions[2].start)\n",
    "                    plt.axvline(cropdimensions[2].stop,\n",
    "                                c=seaborn.color_palette()[3],\n",
    "                                label=cropdimensions[2].stop)\n",
    "                elif d == 1:\n",
    "                    plt.axhline(cropdimensions[0].start,\n",
    "                                c=seaborn.color_palette()[0],\n",
    "                                label=cropdimensions[0].start)\n",
    "                    plt.axhline(cropdimensions[0].stop,\n",
    "                                c=seaborn.color_palette()[1],\n",
    "                                label=cropdimensions[0].stop)\n",
    "                    plt.axvline(cropdimensions[2].start,\n",
    "                                c=seaborn.color_palette()[2],\n",
    "                                label=cropdimensions[2].start)\n",
    "                    plt.axvline(cropdimensions[2].stop,\n",
    "                                c=seaborn.color_palette()[3],\n",
    "                                label=cropdimensions[2].stop)\n",
    "                elif d == 2:\n",
    "                    plt.axhline(cropdimensions[0].start,\n",
    "                                c=seaborn.color_palette()[0],\n",
    "                                label=cropdimensions[0].start)\n",
    "                    plt.axhline(cropdimensions[0].stop,\n",
    "                                c=seaborn.color_palette()[1],\n",
    "                                label=cropdimensions[0].stop)\n",
    "                    plt.axvline(cropdimensions[1].start,\n",
    "                                c=seaborn.color_palette()[2],\n",
    "                                label=cropdimensions[1].start)\n",
    "                    plt.axvline(cropdimensions[1].stop,\n",
    "                                c=seaborn.color_palette()[3],\n",
    "                                label=cropdimensions[1].stop)\n",
    "                plt.title('%s MIP' % direction)\n",
    "                plt.legend(loc='lower right')\n",
    "        else:\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(image, alpha=0.618)\n",
    "            plt.imshow(dask.array.ma.masked_less(thresholded, 1), alpha=0.618, cmap='viridis_r')\n",
    "            plt.axhline(cropdimensions[0].start, c=seaborn.color_palette()[0], label=cropdimensions[0].start)\n",
    "            plt.axhline(cropdimensions[0].stop, c=seaborn.color_palette()[1], label=cropdimensions[0].stop)\n",
    "            plt.axvline(cropdimensions[1].start, c=seaborn.color_palette()[2], label=cropdimensions[1].start)\n",
    "            plt.axvline(cropdimensions[1].stop, c=seaborn.color_palette()[3], label=cropdimensions[1].stop)\n",
    "            plt.title('Original image with thresholded overlay')\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.subplot(122)\n",
    "            plt.imshow(image[cropdimensions])\n",
    "            plt.title('Output')\n",
    "        plt.show()\n",
    "    if verbose:\n",
    "        print('Cropped image from %s to %s' % (image.shape, image[cropdimensions].shape))\n",
    "        print(cropdimensions)\n",
    "    return(image[cropdimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test cropping function\n",
    "# img = Reconstructions[3][1900:]\n",
    "# Cropped = cropper(img, verbose=True, despeckle=True, threshold=55)\n",
    "# for c, direction in enumerate(directions):\n",
    "#     plt.subplot(1, 3, c + 1)\n",
    "#     if 'Axial' in direction:\n",
    "#         plt.imshow(Cropped[Cropped.shape[0] // 2])\n",
    "#     if 'Sagittal' in direction:\n",
    "#         plt.imshow(Cropped[:, Cropped.shape[1] // 2, :])\n",
    "#     if 'Coronal' in direction:\n",
    "#         plt.imshow(Cropped[:, :, Cropped.shape[2] // 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the reconstructions and save them out as .zarr files, too\n",
    "Data['OutputNameRecCrop'] = [os.path.join(f, sample + '_rec_crop.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving cropped teeth to .zarr files',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameRecCrop']):\n",
    "        print('%3s/%3s: Saving to %s' % (c + 1,\n",
    "                                         len(Data),\n",
    "                                         row['OutputNameRecCrop'][len(Root):]))\n",
    "        # Calculate crop with our function and directly write to ZARR file\n",
    "        cropper(Reconstructions[c],\n",
    "                verbose=True,\n",
    "                despeckle=True).to_zarr(row['OutputNameRecCrop'],\n",
    "                                        overwrite=True,\n",
    "                                        compressor=Blosc(cname='zstd',\n",
    "                                                         clevel=3,\n",
    "                                                         shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cropped zarr arrays as reconstructions\n",
    "Reconstructions = [dask.array.from_zarr(file) for file in Data['OutputNameRecCrop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out cropped rec slices\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving out cropped recs',\n",
    "                            total=len(Data)):\n",
    "    os.makedirs(os.path.join(row.Folder, 'rec_crop'),\n",
    "                exist_ok=True)\n",
    "    for d, rec in notebook.tqdm(enumerate(Reconstructions[c]),\n",
    "                                total=len(Reconstructions[c]),\n",
    "                                desc=row.Sample,\n",
    "                                leave=False):\n",
    "        filename = os.path.join(row.Folder,\n",
    "                                'rec_crop', str(row.Sample) + '_rec_crop_%08d.png' % d)\n",
    "        if not os.path.exists(filename):\n",
    "            imageio.imsave(filename, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Middle images', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'], '%s.Middle.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'Mid_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][Reconstructions[c].shape[0] // 2].compute()\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, Reconstructions[c].shape[1] // 2, :].compute()\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c, 'Mid_' + direction] = Reconstructions[c][:, :, Reconstructions[c].shape[2] // 2].compute()\n",
    "            # Save the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, (Data.at[c, 'Mid_' + direction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle slices\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Saving middle images overview', total=len(Data)):\n",
    "    outfilepath = os.path.join(row['Folder'], row['Sample'] + '.MiddleSlices.png')\n",
    "    if not os.path.exists(outfilepath):\n",
    "        for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                          desc=row['Sample'],\n",
    "                                          leave=False,\n",
    "                                          total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['Mid_' + direction])\n",
    "            if d == 0:\n",
    "                plt.axhline(Reconstructions[c].shape[1] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.axvline(Reconstructions[c].shape[2] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[2]))\n",
    "            elif d == 1:\n",
    "                plt.axhline(Reconstructions[c].shape[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(Reconstructions[c].shape[1] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[0]))\n",
    "            else:\n",
    "                plt.axhline(Reconstructions[c].shape[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(Reconstructions[c].shape[2] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[1]))\n",
    "            plt.title('%s, %s' % (row['Sample'],\n",
    "                                  direction + ' Middle slice'))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outfilepath,\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c, row in Data.iterrows():\n",
    "#     print(c, row.Sample, Reconstructions[c].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs of the cropped reconstructions\n",
    "# Put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = [None] * len(Data)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'], '%s.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            try:\n",
    "                # Generate MIP\n",
    "                Data.at[c, 'MIP_' + direction] = Reconstructions[c].max(axis=d).compute()\n",
    "            except ZeroDivisionError:\n",
    "                print('Something does not work for %s, copying uncropped MIP' % row['Sample'])\n",
    "                Data.at[c, 'MIP_' + direction] = Data.at[c, 'MIP_' + direction]\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MIPs\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Saving cropped MIP overview', total=len(Data)):\n",
    "    outfilepath = os.path.join(row['Folder'], row['Sample'] + '.MIPs.png')\n",
    "    if not os.path.exists(outfilepath):\n",
    "        for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                          desc=row['Sample'],\n",
    "                                          leave=False,\n",
    "                                          total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + direction])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "            plt.title('%s, %s' % (row['Sample'],\n",
    "                                  direction + ' MIP'))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outfilepath,\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with the characterization of the bottom part of the tooth it is necessary to have coronal/sagittal slices of the lowest ~3mm. \n",
    "Lets thus extract these slices and write them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many slices are 3.5mm?\n",
    "Data['BottomSlices'] = [int(round(3500 / vs)) for vs in Data['Voxelsize']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop and save out bottom of tooth\n",
    "Data['OutputNameRecApex'] = [os.path.join(f, sample + '_apex_rec.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving apex to .zarr file',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameRecApex']):\n",
    "        print('%3s/%3s: Saving to %s' % (c + 1,\n",
    "                                         len(Data),\n",
    "                                         row['OutputNameRecApex']))\n",
    "        # Save out the bottom part of the tooth after cropping it to its minimal size\n",
    "        cropper(Reconstructions[c][-row.BottomSlices:],\n",
    "                verbose=False,\n",
    "                threshold=50).to_zarr(row['OutputNameRecApex'],\n",
    "                                      overwrite=True,\n",
    "                                      compressor=Blosc(cname='zstd',\n",
    "                                                       clevel=3,\n",
    "                                                       shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the bottom zarr arrays\n",
    "Apexes = [dask.array.from_zarr(file) for file in Data['OutputNameRecApex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reslice the bottom part and write it out\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving resliced bottom part',\n",
    "                            total=len(Data)):\n",
    "    os.makedirs(os.path.join(row.Folder, 'apex_reslice'),\n",
    "                exist_ok=True)\n",
    "    # load the full dataset into memory if empty, otherwise just map it\n",
    "    # Otherwise the reslicing takes *ages*\n",
    "    if os.listdir(os.path.join(row.Folder,\n",
    "                               'apex_reslice')):\n",
    "        apex = Apexes[c]\n",
    "    else:\n",
    "        apex = Apexes[c].compute()\n",
    "    for d in notebook.tqdm(range(Apexes[c].shape[-1]),\n",
    "                           desc=row.Sample,\n",
    "                           leave=False):\n",
    "        filename = os.path.join(row.Folder,\n",
    "                                'apex_reslice',\n",
    "                                str(row.Sample) + '_rec_apex_sagittal_%04d.png' % d)\n",
    "        if not os.path.exists(filename):\n",
    "            imageio.imsave(filename, apex[:, :, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices of the bottom part, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_Apex_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Middle images', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'], '%s.Apex.Middle.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'Mid_Apex_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                Data.at[c, 'Mid_Apex_' + direction] = Apexes[c][Apexes[c].shape[0] // 2].compute()\n",
    "            if 'Sagittal' in direction:\n",
    "                Data.at[c, 'Mid_Apex_' + direction] = Apexes[c][:, Apexes[c].shape[1] // 2, :].compute()\n",
    "            if 'Coronal' in direction:\n",
    "                Data.at[c, 'Mid_Apex_' + direction] = Apexes[c][:, :, Apexes[c].shape[2] // 2].compute()\n",
    "            # Save the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, (Data.at[c, 'Mid_' + direction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show middle slices of the apex\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Saving middle images overview', total=len(Data)):\n",
    "    outfilepath = os.path.join(row['Folder'], row['Sample'] + '.Apex.MiddleSlices.png')\n",
    "    if not os.path.exists(outfilepath):\n",
    "        for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                          desc=row['Sample'],\n",
    "                                          leave=False,\n",
    "                                          total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['Mid_Apex_' + direction])\n",
    "            if d == 0:\n",
    "                plt.axhline(Apexes[c].shape[1] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.axvline(Apexes[c].shape[2] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[2]))\n",
    "            elif d == 1:\n",
    "                plt.axhline(Apexes[c].shape[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(Apexes[c].shape[1] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[0]))\n",
    "            else:\n",
    "                plt.axhline(Apexes[c].shape[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(Apexes[c].shape[2] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um', color=seaborn.color_palette()[1]))\n",
    "            plt.title('%s, %s' % (row['Sample'],\n",
    "                                  direction + ' Middle slice'))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outfilepath,\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_Apex_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='MIPs', total=len(Data)):\n",
    "    for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                      desc=row['Sample'],\n",
    "                                      leave=False,\n",
    "                                      total=len(directions)):\n",
    "        outfilepath = os.path.join(row['Folder'], '%s.Apex.MIP.%s.png' % (row['Sample'], direction))\n",
    "        if os.path.exists(outfilepath):\n",
    "            Data.at[c, 'MIP_Apex_' + direction] = imageio.imread(outfilepath)\n",
    "        else:\n",
    "            # Generate MIP\n",
    "            Data.at[c, 'MIP_Apex_' + direction] = Apexes[c].max(axis=d)\n",
    "            # Save it out\n",
    "            imageio.imwrite(outfilepath, Data.at[c, 'MIP_Apex_' + direction].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MIP slices\n",
    "for c, row in notebook.tqdm(Data.iterrows(), desc='Saving bottom MIP overview', total=len(Data)):\n",
    "    outfilepath = os.path.join(row['Folder'], row['Sample'] + '.Apex.MIPs.png')\n",
    "    if not os.path.exists(outfilepath):\n",
    "        for d, direction in notebook.tqdm(enumerate(directions),\n",
    "                                          desc=row['Sample'],\n",
    "                                          leave=False,\n",
    "                                          total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_Apex_' + direction])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "            plt.title('%s, %s' % (row['Sample'],\n",
    "                                  direction + ' MIP'))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outfilepath,\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in Data['OutputNameRecCrop']:\n",
    "#     print(file)\n",
    "#     dask.array.from_zarr(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partialpart(reconstruction, slicenumber, what='min', numberofslicesaround=50, verbose=False):\n",
    "    '''\n",
    "    Function to get a small part around the selected slice number.\n",
    "    We select the minima to (potentially) show any accessory canals nicely\n",
    "    '''\n",
    "    if what == 'min':\n",
    "        part = numpy.min(reconstruction[slicenumber - numberofslicesaround // 2:\n",
    "                                        slicenumber + numberofslicesaround // 2],\n",
    "                         axis=0)\n",
    "    if what == 'max':\n",
    "        part = numpy.max(reconstruction[slicenumber - numberofslicesaround // 2:\n",
    "                                        slicenumber + numberofslicesaround // 2],\n",
    "                         axis=0)\n",
    "    if what == 'mean':\n",
    "        part = numpy.mean(reconstruction[slicenumber - numberofslicesaround // 2:\n",
    "                                         slicenumber + numberofslicesaround // 2],\n",
    "                          axis=0)\n",
    "    if not what:\n",
    "        part = reconstruction[slicenumber]\n",
    "    if verbose:\n",
    "        plt.imshow(part)\n",
    "    return(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_partialpart(Reconstructions[2], 999, what='min', verbose=True, numberofslicesaround=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_partialpart(Reconstructions[2], 999, what='max', verbose=True, numberofslicesaround=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_partialpart(Reconstructions[2], 999, what='mean', verbose=True, numberofslicesaround=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_partialpart(Reconstructions[2], 999, what=None, verbose=True, numberofslicesaround=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minimum(curve, verbose=False):\n",
    "    '''\n",
    "    Function to detect get the border of enamel/dentin.\n",
    "    We do this by detecting the minimum of the derivative of the smoothed grayvalue curve\n",
    "    Based on https://stackoverflow.com/a/28541805/323100 and some manual tweaking\n",
    "    '''\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    smoothed = lowess(curve, range(len(curve)), return_sorted=False, frac=0.025)\n",
    "    minima = numpy.argmin(numpy.diff(smoothed))\n",
    "    if verbose:\n",
    "        plt.plot(curve, alpha=0.6, label='Input curve')\n",
    "        plt.plot(smoothed, label='LOWESS')\n",
    "        plt.axvline(minima, c='r', label='Border')\n",
    "        plt.legend()\n",
    "    return(minima, smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(whichone, top=0, bottom=50, curve=True, showregion=True, saveslices=True, verbose=False):\n",
    "    '''\n",
    "    According to the discussion on July 6 with Thomas & Andrea, the characterization of the pulpa is done at three levels.\n",
    "    The start of the levels is at the border enamel/dentin.\n",
    "    With a bit of filtering of the gray values of the MIPs along the tooth we find this border and then return three images in this region.\n",
    "    We save the figure and return the four images.\n",
    "    '''\n",
    "    # First we need to detect the border.\n",
    "    # We do this by smoothing the gray value along the tooth, calculating the difference along the axis and taking its minimum\n",
    "    # This should be the border between enamel/dentin\n",
    "    grayvalues = Data['MIP_Sagittal'][whichone].max(axis=1)\n",
    "    if top < 1:\n",
    "        # If we don't set it manually, calculate the border\n",
    "        top, smoothed = get_minimum(grayvalues, verbose)\n",
    "    else:\n",
    "        smoothed = numpy.zeros_like(grayvalues)\n",
    "    # Since we don't want the *very* bottom, we 'fake' slices extraction by generating the range,\n",
    "    # splitting it in three,\n",
    "    # selecting the start of each split as slices and\n",
    "    # add a slice not at the very $bottom\n",
    "    try:\n",
    "        slices = range(top, Reconstructions[whichone].shape[0])\n",
    "        split = numpy.array_split(slices, 3)\n",
    "        slices = [s[0] for s in split]\n",
    "        slices.append(Reconstructions[whichone].shape[0] - bottom)\n",
    "    except IndexError:\n",
    "        print('Something went wrong with the slice calculation')\n",
    "        top = 500\n",
    "        curve = False\n",
    "        slices = range(top, Reconstructions[whichone].shape[0])\n",
    "        split = numpy.array_split(slices, 3)\n",
    "        slices = [s[0] for s in split]\n",
    "        slices.append(Reconstructions[whichone].shape[0] - bottom)\n",
    "    if verbose:\n",
    "        print('The scan has %s slices' % Reconstructions[whichone].shape[0])\n",
    "        print('We found the enamel/dentin border at slice %s' % top)\n",
    "        print('We split the distance from slice', end=' ')\n",
    "        print('%s to %s in three parts (e.g. %s//3)' % (top,\n",
    "                                                        Reconstructions[whichone].shape[0],\n",
    "                                                        Reconstructions[whichone].shape[0] - top))\n",
    "        print('We are returning slice', end=' ')\n",
    "        for i in slices[:-1]:\n",
    "            print(i, end=', ')\n",
    "        print('and %s for the pulpa characterization' % slices[-1])\n",
    "    # Based on https://matplotlib.org/tutorials/intermediate/gridspec.html\n",
    "    fig = plt.figure(constrained_layout=True)\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    fig_mip_s = fig.add_subplot(gs[:, 0])\n",
    "    plt.imshow(Data['MIP_Sagittal'][whichone])\n",
    "    if curve:\n",
    "        plt.plot(grayvalues / 2**8 * Data['MIP_Sagittal'][whichone].shape[1],\n",
    "                 range(Data['MIP_Sagittal'][whichone].shape[0]),\n",
    "                 alpha=0.618)\n",
    "        plt.plot(smoothed / 2**8 * Data['MIP_Sagittal'][whichone].shape[1],\n",
    "                 range(Data['MIP_Sagittal'][whichone].shape[0]),\n",
    "                 alpha=0.618)\n",
    "    for i in slices:\n",
    "        plt.axhline(i, c='r')\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "    fig_mip_s.set_title('Sagittal MIP\\nSlices and grey value profile')\n",
    "    fig_mip_c = fig.add_subplot(gs[:, 1])\n",
    "    plt.imshow(Data['MIP_Coronal'][whichone])\n",
    "    if curve:\n",
    "        plt.plot(grayvalues / 2**8 * Data['MIP_Coronal'][whichone].shape[1],\n",
    "                 range(Data['MIP_Sagittal'][whichone].shape[0]),\n",
    "                 alpha=0.618)\n",
    "        plt.plot(smoothed / 2**8 * Data['MIP_Coronal'][whichone].shape[1],\n",
    "                 range(Data['MIP_Coronal'][whichone].shape[0]),\n",
    "                 alpha=0.618)\n",
    "    for i in slices:\n",
    "        plt.axhline(i, c='r')\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "    fig_mip_c.set_title('Coronal MIP\\nSlices and grey value profile')\n",
    "    for i in range(4):\n",
    "        if i < 2:\n",
    "            fig_slc = fig.add_subplot(gs[0, i + 2])\n",
    "        else:\n",
    "            fig_slc = fig.add_subplot(gs[1, i])\n",
    "        if showregion:\n",
    "            nsa = 40\n",
    "            plt.imshow(get_partialpart(Reconstructions[whichone],\n",
    "                                       slices[i],\n",
    "                                       numberofslicesaround=nsa))\n",
    "            fig_slc.set_title('Minima of %s slices around slice %s' % (nsa, slices[i]))\n",
    "        else:\n",
    "            plt.imshow(Reconstructions[whichone][slices[i]])\n",
    "            fig_slc.set_title('Slice %s' % slices[i])\n",
    "        plt.gca().add_artist(ScaleBar(row['Voxelsize'], 'um'))\n",
    "        plt.axis('off')\n",
    "    if verbose:\n",
    "        print('Saving figure to %s' % os.path.join(Data.Folder[whichone],\n",
    "                                                   Data.Sample[whichone] + 'ExtractedSlices.png'))\n",
    "    plt.suptitle(Data.Sample[whichone])\n",
    "    plt.savefig(os.path.join(Data.Folder[whichone],\n",
    "                             Data.Sample[whichone] + '.ExtractedSlices.png'),\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()\n",
    "    if saveslices:\n",
    "        # remove files from prior runs\n",
    "        for file in glob.glob(os.path.join(Data.Folder[whichone],\n",
    "                                           Data.Sample[whichone] + '.ExtractedSlice.????.png')):\n",
    "            os.remove(file)\n",
    "        # Save out the slices if requested\n",
    "        for slc in slices:\n",
    "            imageio.imsave(os.path.join(Data.Folder[whichone],\n",
    "                                        Data.Sample[whichone] + '.ExtractedSlice.%04d.png' % slc),\n",
    "                           Reconstructions[whichone][slc])\n",
    "    return(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Data['ExtractedSlices'] = ''\n",
    "for c, row in Data.iterrows():\n",
    "    if not os.path.exists(os.path.join(row.Folder, row.Sample + '.ExtractedSlices.png')):\n",
    "        print('%3s/%3s: Detecting enamel/dentin border for %s and saving output' % (c + 1, len(Data), row.Sample))\n",
    "        Data.at[c, 'ExtractedSlices'] = splitter(c)\n",
    "    else:\n",
    "        print('%3s/%3s: Detecting enamel/dentin border for %s' % (c + 1, len(Data), row.Sample))\n",
    "        Data.at[c, 'ExtractedSlices'] = splitter(c, saveslices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data[Data['Sample'] == 'Tooth102']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For several samples the EDB detection doesn't work automatically\n",
    "# We can do it manually by setting the 'top' variable\n",
    "Data.at[0, 'ExtractedSlices'] = splitter(0, top=850)\n",
    "Data.at[5, 'ExtractedSlices'] = splitter(5, top=825)\n",
    "Data.at[9, 'ExtractedSlices'] = splitter(9, top=1025)\n",
    "Data.at[11, 'ExtractedSlices'] = splitter(11, top=700)\n",
    "Data.at[14, 'ExtractedSlices'] = splitter(14, top=940)\n",
    "Data.at[17, 'ExtractedSlices'] = splitter(17, top=945)\n",
    "Data.at[19, 'ExtractedSlices'] = splitter(19, top=900)\n",
    "Data.at[23, 'ExtractedSlices'] = splitter(23, top=1100)\n",
    "Data.at[28, 'ExtractedSlices'] = splitter(28, top=950)\n",
    "Data.at[30, 'ExtractedSlices'] = splitter(30, top=1050)\n",
    "Data.at[32, 'ExtractedSlices'] = splitter(32, top=1050)\n",
    "Data.at[39, 'ExtractedSlices'] = splitter(39, top=875)\n",
    "Data.at[43, 'ExtractedSlices'] = splitter(43, top=775)\n",
    "Data.at[44, 'ExtractedSlices'] = splitter(44, top=950)\n",
    "Data.at[45, 'ExtractedSlices'] = splitter(45, top=910)\n",
    "Data.at[50, 'ExtractedSlices'] = splitter(50, top=750)\n",
    "Data.at[54, 'ExtractedSlices'] = splitter(54, top=1075)\n",
    "Data.at[57, 'ExtractedSlices'] = splitter(57, top=875)\n",
    "Data.at[58, 'ExtractedSlices'] = splitter(58, top=775)\n",
    "Data.at[59, 'ExtractedSlices'] = splitter(59, top=750)\n",
    "Data.at[60, 'ExtractedSlices'] = splitter(60, top=1000)\n",
    "Data.at[61, 'ExtractedSlices'] = splitter(61, top=1100)\n",
    "Data.at[65, 'ExtractedSlices'] = splitter(65, top=1075)\n",
    "Data.at[68, 'ExtractedSlices'] = splitter(68, top=940)\n",
    "Data.at[70, 'ExtractedSlices'] = splitter(70, top=860)\n",
    "Data.at[73, 'ExtractedSlices'] = splitter(73, top=700)\n",
    "Data.at[74, 'ExtractedSlices'] = splitter(74, top=1000)\n",
    "Data.at[76, 'ExtractedSlices'] = splitter(76, top=1000)\n",
    "Data.at[79, 'ExtractedSlices'] = splitter(79, top=1000)\n",
    "Data.at[81, 'ExtractedSlices'] = splitter(81, top=1010)\n",
    "Data.at[86, 'ExtractedSlices'] = splitter(86, top=825)\n",
    "Data.at[94, 'ExtractedSlices'] = splitter(94, top=850)\n",
    "Data.at[95, 'ExtractedSlices'] = splitter(95, top=1000)\n",
    "Data.at[97, 'ExtractedSlices'] = splitter(97, top=775)\n",
    "Data.at[98, 'ExtractedSlices'] = splitter(98, top=875)\n",
    "Data.at[99, 'ExtractedSlices'] = splitter(99, top=650)\n",
    "Data.at[101, 'ExtractedSlices'] = splitter(101, top=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulponator(image, verbose=False):\n",
    "    '''Function to extract the pulpa from the tooth'''\n",
    "    # Load image into memory (thresholding et al. does not work on Dask array)\n",
    "    # https://github.com/dask/dask-image/issues/100\n",
    "    image = image.compute()\n",
    "    # Very crudely threshold the tooth from the background\n",
    "    thresholded = image > skimage.filters.threshold_otsu(image)\n",
    "    # The thresholded tooth is not the only feature in the image\n",
    "    # If we remove the border on the inverted image, we get the stuff that is *not* connected to the border\n",
    "    stuffinside = skimage.segmentation.clear_border(thresholded == False)\n",
    "    # Let's remove some speckles (stuff larger than 16 px is left)\n",
    "    # These are small speckles left over from the thresholding\n",
    "    speckleless = skimage.morphology.remove_small_objects(stuffinside, min_size=8**2)\n",
    "    # Now close holes in the pulpa region\n",
    "    pulpa = skimage.morphology.remove_small_holes(speckleless, area_threshold=10**2)\n",
    "    if verbose:\n",
    "        plt.subplot(141)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title('Input image')\n",
    "        plt.subplot(142)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(numpy.ma.masked_equal(thresholded, 0), cmap='viridis_r')\n",
    "        plt.title('Thresholded')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(143)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(numpy.ma.masked_equal(stuffinside, 0), cmap='viridis_r')\n",
    "        plt.title('Uncleaned pulpa')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(144)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(numpy.ma.masked_equal(pulpa, 0), cmap='viridis_r')\n",
    "        plt.title('Cleaned pulpa over original')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return(pulpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec = Reconstructions[99][666]\n",
    "# _=pulponator(rec, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pulpa from the *full* teeth\n",
    "Data['OutputNamePulpa'] = [os.path.join(f, sample + '_pulpa.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Extracting pulpa, saving to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNamePulpa']):\n",
    "        print('%3s/%3s: Extracting pulpa and saving to %s' % (c + 1,\n",
    "                                                              len(Data),\n",
    "                                                              row['OutputNamePulpa'][len(Root):]))\n",
    "        Pulpa = [pulponator(rec) for rec in Reconstructions[c]]\n",
    "        Pulpa = dask.array.stack(Pulpa[:])\n",
    "        Pulpa.to_zarr(row['OutputNamePulpa'],\n",
    "                      overwrite=True,\n",
    "                      compressor=Blosc(cname='zstd',\n",
    "                                       clevel=3,\n",
    "                                       shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pulpa zarr arrays\n",
    "Pulpas = [dask.array.from_zarr(file) for file in Data['OutputNamePulpa']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the pulpa slices\n",
    "verbose = False\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving out pulpa slices',\n",
    "                            total=len(Data)):\n",
    "    os.makedirs(os.path.join(row.Folder, 'pulpa'),\n",
    "                exist_ok=True)\n",
    "    for d, rec in notebook.tqdm(enumerate(Pulpas[c]),\n",
    "                                desc=row.Sample,\n",
    "                                total=len(Pulpas[c]),\n",
    "                                leave=False):\n",
    "        filename = os.path.join(row.Folder,\n",
    "                                'pulpa', str(row.Sample) + '_pulpa_%08d.png' % d)\n",
    "        if not os.path.exists(filename):\n",
    "            if verbose:\n",
    "                plt.imshow(rec)\n",
    "                plt.title('%s, Slice %s' % (row.Sample, d))\n",
    "                plt.show()\n",
    "            imageio.imsave(filename, rec.astype('uint8') * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pulpa from the *apex* of the teeth\n",
    "Data['OutputNameApexPulpa'] = [os.path.join(f, sample + '_apex_pulpa.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Extracting pulpa, saving to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameApexPulpa']):\n",
    "        print('%3s/%3s: Extracting pulpa and saving to %s' % (c + 1,\n",
    "                                                              len(Data),\n",
    "                                                              row['OutputNameApexPulpa'][len(Root):]))\n",
    "        Pulpa = [pulponator(rec) for rec in Apexes[c]]\n",
    "        Pulpa = dask.array.stack(Pulpa[:])\n",
    "        Pulpa.to_zarr(row['OutputNameApexPulpa'],\n",
    "                      overwrite=True,\n",
    "                      compressor=Blosc(cname='zstd',\n",
    "                                       clevel=3,\n",
    "                                       shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pulpa zarr arrays\n",
    "ApexPulpas = [dask.array.from_zarr(file) for file in Data['OutputNameApexPulpa']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the pulpa slices of the apexes\n",
    "verbose = False\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving out pulpa slices',\n",
    "                            total=len(Data)):\n",
    "    os.makedirs(os.path.join(row.Folder, 'apex_pulpa'),\n",
    "                exist_ok=True)\n",
    "    for d, rec in notebook.tqdm(enumerate(ApexPulpas[c]),\n",
    "                                desc=row.Sample,\n",
    "                                total=len(ApexPulpas[c]),\n",
    "                                leave=False):\n",
    "        filename = os.path.join(row.Folder,\n",
    "                                'apex_pulpa', str(row.Sample) + '_pulpa_%08d.png' % d)\n",
    "        if not os.path.exists(filename):\n",
    "            if verbose:\n",
    "                plt.imshow(rec)\n",
    "                plt.title('%s, Slice %s' % (row.Sample, d))\n",
    "                plt.show()\n",
    "            imageio.imsave(filename, rec.astype('uint8') * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brisenoizer(whichone, verbose=False):\n",
    "    \"\"\"\n",
    "    Based on the slices we classify the tooth according to [@doi:10.1016/j.joen.2015.09.007]\n",
    "    \"\"\"\n",
    "    # Load the four images from the pulpa\n",
    "    slices = Data['ExtractedSlices'][whichone]\n",
    "    if verbose:\n",
    "        print('%s: Looking at slices %s' % (Data['Sample'][whichone], set(slices)))\n",
    "        plt.rcParams['figure.figsize'] = (16, 4.5)\n",
    "    briseno = []\n",
    "    for c, slice in enumerate(slices):\n",
    "        labels, num_features = scipy.ndimage.label(Pulpas[whichone][slice])\n",
    "        briseno.append(num_features)\n",
    "        if verbose:\n",
    "            plt.subplot(1, 4, c + 1)\n",
    "            plt.imshow(Reconstructions[whichone][slice])\n",
    "            plt.imshow(dask.array.ma.masked_less(Pulpas[whichone][slice], 1), cmap='viridis_r')\n",
    "            if not c:\n",
    "                plt.title('%s, Slice %s: %s' % (Data['Sample'][whichone], slice, num_features))\n",
    "            else:\n",
    "                plt.title('Slice %s: %s' % (slice, num_features))\n",
    "            plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichone], 'um'))\n",
    "            plt.axis('off')\n",
    "    if verbose:\n",
    "        plt.suptitle(Data.Sample[whichone])\n",
    "        plt.savefig(os.path.join(Data.Folder[whichone],\n",
    "                                 Data.Sample[whichone] + '.Briseno.png'),\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()\n",
    "    plt.rcParams['figure.figsize'] = (16, 9)\n",
    "    return(briseno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brisenoizer(11, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Briseno'] = ''\n",
    "for c, row in Data.iterrows():\n",
    "    if not os.path.exists(os.path.join(row.Folder, row.Sample + '.Briseno.png')):\n",
    "        print('%3s/%3s: Extracting Briseo classification for %s and saving output' % (c + 1,\n",
    "                                                                                       len(Data),\n",
    "                                                                                       row.Sample))\n",
    "        Data.at[c, 'Briseno'] = brisenoizer(c, verbose=True)\n",
    "    else:\n",
    "        print('%3s/%3s: Detecting enamel/dentin border for %s' % (c + 1,\n",
    "                                                                  len(Data),\n",
    "                                                                  row.Sample))\n",
    "        Data.at[c, 'Briseno'] = brisenoizer(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDT of *apex* of pulpas\n",
    "Data['OutputNameEDT'] = [os.path.join(f, sample + '_apex_edt.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Extracting EDT of apex, saving to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameEDT']):\n",
    "        print('%3s/%3s: Calculating EDT and saving to %s' % (c + 1,\n",
    "                                                             len(Data),\n",
    "                                                             row['OutputNameEDT'][len(Root):]))\n",
    "        EDT = scipy.ndimage.morphology.distance_transform_edt(ApexPulpas[c])\n",
    "        EDT = dask.array.stack(EDT[:])\n",
    "        EDT.to_zarr(row['OutputNameEDT'],\n",
    "                    overwrite=True,\n",
    "                    compressor=Blosc(cname='zstd',\n",
    "                                     clevel=3,\n",
    "                                     shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the EDT zarr arrays\n",
    "EDTs = [dask.array.from_zarr(file) for file in Data['OutputNameEDT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the EDT slices\n",
    "verbose = False\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving out EDT slices',\n",
    "                            total=len(Data)):\n",
    "    os.makedirs(os.path.join(row.Folder, 'apex_edt'),\n",
    "                exist_ok=True)\n",
    "    for d, rec in notebook.tqdm(enumerate(EDTs[c]),\n",
    "                                desc=row.Sample,\n",
    "                                total=len(EDTs[c]),\n",
    "                                leave=False):\n",
    "        filename = os.path.join(row.Folder,\n",
    "                                'apex_edt', str(row.Sample) + '_edt_%08d.png' % d)\n",
    "        if not os.path.exists(filename):\n",
    "            if verbose:\n",
    "                plt.imshow(rec)\n",
    "                plt.title('%s, Slice %s' % (row.Sample, d))\n",
    "                plt.show()\n",
    "            imageio.imsave(filename, rec.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeletonization of only the apex of the pulpas\n",
    "Data['OutputNameSkeleton'] = [os.path.join(f, sample + '_apex_skeleton.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Skeletonization of apex, saving to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameSkeleton']):\n",
    "        print('%3s/%3s: Skeletonizing apex and saving to %s' % (c + 1,\n",
    "                                                                len(Data),\n",
    "                                                                row['OutputNameSkeleton'][len(Root):]))\n",
    "        Skeleton = skimage.morphology.skeletonize_3d(ApexPulpas[c].astype('bool').compute())\n",
    "        Skeleton = dask.array.stack(Skeleton[:].astype('bool'))\n",
    "        Skeleton.to_zarr(row['OutputNameSkeleton'],\n",
    "                         overwrite=True,\n",
    "                         compressor=Blosc(cname='zstd',\n",
    "                                          clevel=3,\n",
    "                                          shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pupla zarr arrays\n",
    "Skeletons = [dask.array.from_zarr(file) for file in Data['OutputNameSkeleton']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge EDT and skeleton, so we have the distance on the skeleton\n",
    "Data['OutputNameSkelDist'] = [os.path.join(f, sample + '_apex_skeldist.zarr') for f, sample in zip(Data.Folder, Data.Sample)]\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Merging skel & EDT, saving to .zarr',\n",
    "                            total=len(Data)):\n",
    "    if not os.path.exists(row['OutputNameSkelDist']):\n",
    "        print('%3s/%3s: Merging skeleton and EDT and saving to %s' % (c + 1,\n",
    "                                                                      len(Data),\n",
    "                                                                      row['OutputNameSkelDist'][len(Root):]))\n",
    "        dask.array.multiply(EDTs[c], Skeletons[c]).to_zarr(row['OutputNameSkelDist'],\n",
    "                                                           overwrite=True,\n",
    "                                                           compressor=Blosc(cname='zstd',\n",
    "                                                                            clevel=3,\n",
    "                                                                            shuffle=Blosc.BITSHUFFLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the skeldist zarr arrays\n",
    "SkelDists = [dask.array.from_zarr(file) for file in Data['OutputNameSkelDist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the SkelDist slices\n",
    "verbose = False\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving out SkelDist slices',\n",
    "                            total=len(Data)):\n",
    "    os.makedirs(os.path.join(row.Folder, 'apex_skeldist'),\n",
    "                exist_ok=True)\n",
    "    for d, rec in notebook.tqdm(enumerate(SkelDists[c]),\n",
    "                                desc=row.Sample,\n",
    "                                total=len(SkelDists[c]),\n",
    "                                leave=False):\n",
    "        filename = os.path.join(row.Folder,\n",
    "                                'apex_skeldist', str(row.Sample) + '_skeldist_%08d.png' % d)\n",
    "        if not os.path.exists(filename):\n",
    "            if verbose:\n",
    "                plt.imshow(rec)\n",
    "                plt.title('%s, Slice %s' % (row.Sample, d))\n",
    "                plt.show()\n",
    "            imageio.imsave(filename, rec.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one sample and slice to show what we did\n",
    "whichone = 1\n",
    "whichslice = 111\n",
    "print(Data['Sample'][whichone])\n",
    "plt.subplot(221)\n",
    "plt.imshow(Apexes[whichone][whichslice])\n",
    "plt.imshow(ApexPulpas[whichone][whichslice], alpha=0.618, cmap='viridis')\n",
    "plt.title(ApexPulpas[whichone][whichslice].max().compute())\n",
    "plt.subplot(222)\n",
    "plt.imshow(Apexes[whichone][whichslice])\n",
    "plt.imshow(EDTs[whichone][whichslice], alpha=0.618, cmap='viridis')\n",
    "plt.title(EDTs[whichone][whichslice].max().compute())\n",
    "plt.subplot(223)\n",
    "plt.imshow(Apexes[whichone][whichslice])\n",
    "plt.imshow(Skeletons[whichone][whichslice], alpha=0.618, cmap='viridis')\n",
    "plt.title(Skeletons[whichone][whichslice].max().compute())\n",
    "plt.subplot(224)\n",
    "plt.imshow(Apexes[whichone][whichslice])\n",
    "plt.imshow(SkelDists[whichone][whichslice], alpha=0.618, cmap='viridis')\n",
    "plt.title(SkelDists[whichone][whichslice].max().compute())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itkwidgets\n",
    "from itkwidgets import view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D view of bottom part\n",
    "view(Reconstructions[whichone][-Data['BottomSlices'][whichone]:].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D view of SkeletonDistance\n",
    "view(SkelDists[whichone].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay teeth and EDT of pulpa\n",
    "TeethPulpa = numpy.maximum(EDTs[whichone] * 255,\n",
    "                           Apexes[whichone]).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(TeethPulpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one sample and slice to show what we did in 2D\n",
    "whichone = 1\n",
    "# for whichslice in range(100, 200):\n",
    "for whichslice in range(100, 102):\n",
    "    plt.imshow(Apexes[whichone][whichslice], alpha=0.618)\n",
    "    plt.imshow(numpy.ma.masked_equal(EDTs[whichone][whichslice], 0), cmap='viridis', alpha=0.618)\n",
    "    plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichone], 'um'))\n",
    "    plt.title('%s: EDT on axial slice %s' % (Data['Sample'][whichone], whichslice))\n",
    "    # plt.axis('off')\n",
    "    plt.savefig(os.path.join(Data.Folder[whichone],\n",
    "                             Data.Sample[whichone] + '.EDT.Axial.%s.png' % whichslice),\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show resliced view\n",
    "whichone = 1\n",
    "where = 180\n",
    "# for where in range(100, 250):\n",
    "for where in range(100, 102):\n",
    "    plt.imshow(Apexes[whichone][:, :, where], alpha=0.618)\n",
    "    plt.imshow(numpy.ma.masked_equal(EDTs[whichone][:, :, where], 0), cmap='viridis', alpha=0.618)\n",
    "    plt.savefig('EDT.png',\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    plt.title(Data['Sample'][whichone])\n",
    "    plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichone], 'um'))\n",
    "    plt.title('%s: EDT on coronal slice %s' % (Data['Sample'][whichone], where))\n",
    "    # plt.axis('off')\n",
    "    plt.savefig(os.path.join(Data.Folder[whichone],\n",
    "                             Data.Sample[whichone] + '.EDT.Coronal.%s.png' % where),\n",
    "                transparent=True,\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select another sample\n",
    "whichone = 0\n",
    "apex = Apexes[whichone]\n",
    "pulpa = ApexPulpas[whichone]\n",
    "edt = EDTs[whichone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tooth and pulpa\n",
    "whichslice = 150\n",
    "plt.subplot(121)\n",
    "plt.imshow(apex[whichslice])\n",
    "plt.title([Data['Sample'][whichone]])\n",
    "plt.subplot(122)\n",
    "plt.imshow(pulpa[whichslice])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pulpa area (e.g. just sum up each slice, since we have them binarized/segmented)\n",
    "Data['PulpArea'] = ''\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Calculating pulpa area',\n",
    "                            total=len(Data)):\n",
    "    Data.at[c, 'PulpArea'] = [p.sum().compute() for p in ApexPulpas[c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show pulpa areas slices\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving pulpa area plot',\n",
    "                            total=len(Data)):\n",
    "    plt.plot(row.PulpArea, label=row.Sample)\n",
    "plt.xlabel('Slices')\n",
    "plt.ylabel('Sum of pulpa area')\n",
    "plt.legend()\n",
    "plt.title('All %s pulpa areas' % len(Data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out pulpa areas\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving pulpa area plot',\n",
    "                            total=len(Data)):\n",
    "    outfilepath = os.path.join(row['Folder'], row['Sample'] + '.PulpArea.png')\n",
    "    if not os.path.exists(outfilepath):\n",
    "        plt.plot(row.PulpArea)\n",
    "        plt.title('Pulpa area of bottom of %s' % row.Sample)\n",
    "        plt.xlim([0, row.BottomSlices])\n",
    "        plt.xlabel('Slices')\n",
    "        plt.ylabel('Sum of pulpa area')\n",
    "        plt.savefig(outfilepath,\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Recs and Pulpas\n",
    "# There's a dask.ma.masked.where* function that we could use, but like so it works just fine.\n",
    "# The result is the apexes with '0' where we have thresholded the pulpa.\n",
    "# This makes it possible to merge the EDT into it in a subsequent step.\n",
    "MergePulpaBlank = [dask.array.multiply(p == False, a) for p, a in zip(ApexPulpas, Apexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Recs with blanked pulpas and EDT\n",
    "scaled = False\n",
    "if scaled:\n",
    "    # We scale the EDT with the pixel size, so Andrea doesn't have to calculate too much.\n",
    "    MergePulpaEDT = [dask.array.add(mpb, edt * vs) for mpb, edt, vs in zip(MergePulpaBlank, EDTs, Data.Voxelsize)]\n",
    "else:\n",
    "    # We could scale it, but then we clip at 255 (8bit)\n",
    "    # This is because voxelsize * radius is larger than 255\n",
    "    # Run the cell below to check.\n",
    "    MergePulpaEDT = [dask.array.add(mpb, edt) for mpb, edt in zip(MergePulpaBlank, EDTs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print('Tooth, EDT, Voxelsize, EDT * vs')\n",
    "# for c, i in enumerate(zip(EDTs, Data.Voxelsize)):\n",
    "#     m = i[0].max().compute()\n",
    "#     if m * i[1] > 255:\n",
    "#         print(Data.Sample[c], round(m), i[1], m * i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c, row in notebook.tqdm(Data.iterrows()):\n",
    "#     plt.imshow(MergePulpaEDT[c][222])\n",
    "#     plt.title(row.Sample)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and save out rec and pulpa slices\n",
    "verbose = False\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving out merged Rec + EDT slices',\n",
    "                            total=len(Data)):\n",
    "    os.makedirs(os.path.join(row.Folder, 'apex_merge_rec_edt'),\n",
    "                exist_ok=True)\n",
    "    for d, rec in notebook.tqdm(enumerate(MergePulpaEDT[c]),\n",
    "                                desc=row.Sample,\n",
    "                                total=len(SkelDists[c]),\n",
    "                                leave=False):\n",
    "        filename = os.path.join(row.Folder,\n",
    "                                'apex_merge_rec_edt', str(row.Sample) + '_rec_merge_edt_%08d.png' % d)\n",
    "        if not os.path.exists(filename):\n",
    "            if verbose:\n",
    "                plt.imshow(rec)\n",
    "                plt.title('%s, Slice %s' % (row.Sample, d))\n",
    "                plt.show()\n",
    "            imageio.imsave(filename, rec.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reslice the bottom part and write it out\n",
    "for c, row in notebook.tqdm(Data.iterrows(),\n",
    "                            desc='Saving out Rec + EDT slices, resliced',\n",
    "                            total=len(Data)):\n",
    "    os.makedirs(os.path.join(row.Folder, 'apex_merge_rec_edt_reslice'),\n",
    "                exist_ok=True)\n",
    "    # load the full dataset into memory if empty, otherwise just map it\n",
    "    if os.listdir(os.path.join(row.Folder,\n",
    "                               'apex_merge_rec_edt_reslice')):\n",
    "        mep = MergePulpaEDT[c]\n",
    "    else:\n",
    "        mep = MergePulpaEDT[c].compute()\n",
    "    for d in notebook.tqdm(range(MergePulpaEDT[c].shape[-1]),\n",
    "                           desc=row.Sample,\n",
    "                           leave=False):\n",
    "        filename = os.path.join(row.Folder,\n",
    "                                'apex_merge_rec_edt_reslice',\n",
    "                                str(row.Sample) + '_rec_merge_edt_sagittal_%04d.png' % d)\n",
    "        if not os.path.exists(filename):\n",
    "            imageio.imsave(filename, mep[:, :, d].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TeethPulpa = numpy.maximum(Pulpas[whichone][1:] * 255, Reconstructions[whichone][1:]).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(TeethPulpa.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(Reconstructions[whichone].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(Pulpas[whichone].astype('uint8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
